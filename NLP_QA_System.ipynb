{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NLP Project Q/A System\n",
    "\n",
    "Author: Taniya Riar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import ne_chunk\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import codecs\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import spacy\n",
    "from heapq import nlargest \n",
    "from nltk.corpus import wordnet as wn\n",
    "import string\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_words(text,fname=\"\"):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        words.append(token.text)\n",
    "    non_stop_sentence = [w for w in words if not w in stop_words] \n",
    "    return(non_stop_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_lemma(text):\n",
    "    lemma_words =[]\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        lemma_words.append(token.lemma_)\n",
    "    return(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_pos(text):\n",
    "    POS_list = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        POS_list.append(str(token.text+'_'+token.tag_))\n",
    "    return POS_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_ne(text):\n",
    "    NER_list = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc.ents:\n",
    "            NER_list.append(token.text+\"_\"+token.label_)\n",
    "    return(NER_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_parse_tree(text):\n",
    "    parse_tree = []\n",
    "    doc =nlp(text)\n",
    "    for token in doc:\n",
    "        parse_tree.append(str(token.text+\"_\"+token.dep_))\n",
    "    return(parse_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#implement relation extraction from wordnet\n",
    "def extract_relations(word_score):\n",
    "    word_relations_feature = {}\n",
    "    for key in word_score:\n",
    "        syn = wn.synsets(key)\n",
    "        r_j = []\n",
    "        if len(syn) != 0:\n",
    "            for s in syn:\n",
    "                synonyms = s.lemma_names()\n",
    "                definition = s.definition()\n",
    "                hyponyms = sorted([lemma.name() for synset in s.hyponyms() for lemma in synset.lemmas()])\n",
    "                hypernyms = [lemma.name() for synset in  s.hypernyms() for lemma in synset.lemmas()]\n",
    "                meronyms = s.part_meronyms()\n",
    "                holonyms = s.part_holonyms()\n",
    "                r_j.append({'synset':s,'relations':{'def':definition,'syn':synonyms,'hypo':hyponyms,'hyper':hypernyms,'mero':meronyms,'holo':holonyms}})\n",
    "        else:\n",
    "            word_relations_feature[key] = []\n",
    "        word_relations_feature[key] = r_j         \n",
    "    return word_relations_feature    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Wikipedia Doc directory\n",
      "File processing ------->  AbrahamLincoln.txt\n",
      "Number of Lines in file ---->  651\n",
      "File processing ------->  Amazon_com.txt\n",
      "Number of Lines in file ---->  252\n",
      "File processing ------->  AppleInc.txt\n",
      "Number of Lines in file ---->  490\n",
      "File processing ------->  AT_T.txt\n",
      "Number of Lines in file ---->  161\n",
      "File processing ------->  Berkshire_Hathaway.txt\n",
      "Number of Lines in file ---->  234\n",
      "File processing ------->  China.txt\n",
      "Number of Lines in file ---->  657\n",
      "File processing ------->  CitiGroup.txt\n",
      "Number of Lines in file ---->  331\n",
      "File processing ------->  Dallas.txt\n",
      "Number of Lines in file ---->  567\n",
      "File processing ------->  ElonMusk.txt\n",
      "Number of Lines in file ---->  166\n",
      "File processing ------->  Europe.txt\n",
      "Number of Lines in file ---->  471\n",
      "File processing ------->  ExxonMobil.txt\n",
      "Number of Lines in file ---->  307\n",
      "File processing ------->  GeorgeWashington.txt\n",
      "Number of Lines in file ---->  630\n",
      "File processing ------->  IBM.txt\n",
      "Number of Lines in file ---->  240\n",
      "File processing ------->  India.txt\n",
      "Number of Lines in file ---->  401\n",
      "File processing ------->  JPMorganChase.txt\n",
      "Number of Lines in file ---->  262\n",
      "File processing ------->  LaurenePowellJobs.txt\n",
      "Number of Lines in file ---->  47\n",
      "File processing ------->  MahatmaGandhi.txt\n",
      "Number of Lines in file ---->  870\n",
      "File processing ------->  MelindaGates.txt\n",
      "Number of Lines in file ---->  54\n",
      "File processing ------->  NYC.txt\n",
      "Number of Lines in file ---->  670\n",
      "File processing ------->  OprahWinfrey.txt\n",
      "Number of Lines in file ---->  385\n",
      "File processing ------->  Rchardson_Texas.txt\n",
      "Number of Lines in file ---->  207\n",
      "File processing ------->  SteveJobs.txt\n",
      "Number of Lines in file ---->  639\n",
      "File processing ------->  TeslaInc.txt\n",
      "Number of Lines in file ---->  419\n",
      "File processing ------->  Texas.txt\n",
      "Number of Lines in file ---->  768\n",
      "File processing ------->  UnitedStates.txt\n",
      "Number of Lines in file ---->  815\n",
      "File processing ------->  UTD.txt\n",
      "Number of Lines in file ---->  333\n",
      "File processing ------->  Walmart.txt\n",
      "Number of Lines in file ---->  504\n",
      "File processing ------->  WarrenBuffet.txt\n",
      "Number of Lines in file ---->  355\n",
      "File processing ------->  Washington_DC.txt\n",
      "Number of Lines in file ---->  374\n",
      "File processing ------->  Washington_State.txt\n",
      "Number of Lines in file ---->  379\n",
      "541.6905190944672\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "start = time.time()\n",
    "path = input(\"Enter Wikipedia Doc directory\")\n",
    "if path == \"\":\n",
    "    path = \"<give some default path name>\"\n",
    "    \n",
    "os.chdir(path)\n",
    "\n",
    "data_text_1 = []\n",
    "corpus_lemma =[]\n",
    "for filename in glob.glob(\"*.txt\"):\n",
    "    sentences=[]\n",
    "    doc = {}\n",
    "    if filename != \"MelindaGates.txt\":\n",
    "        f = codecs.open(filename,'r','utf-8-sig') \n",
    "    else:\n",
    "        f = codecs.open(filename,'r','ISO-8859-1')\n",
    "    file = f.read()\n",
    "    print(\"File processing -------> \",filename)\n",
    "    for l in sent_tokenize(file):\n",
    "        if l.strip():\n",
    "            \n",
    "            w_j_obj= extract_words(l,filename)\n",
    "            \n",
    "            l_j_obj= extract_lemma(l)\n",
    "            \n",
    "            p_j_obj= extract_pos(l)\n",
    "            \n",
    "            ne_j_obj= extract_ne(l)\n",
    "            \n",
    "            dp_j_obj = extract_parse_tree(l)\n",
    "            \n",
    "            sentences.append({\"sentence\":l,\"lemma\":l_j_obj,\"pos\":p_j_obj,\"ner\": ne_j_obj,\"parse_tree\":dp_j_obj})\n",
    "           \n",
    "            corpus_lemma.extend(l_j_obj)\n",
    "    doc = {'doc':filename,'sentences':sentences}\n",
    "    print(\"Number of Lines in file ----> \",len(sent_tokenize(file)))\n",
    "    data_text_1.append(doc)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Already Exists\n",
      "Corpus File path -> C:/Users/taniy/Desktop/Spring19/CS6320-NLP/Project/WikipediaArticles/WikipediaArticles/result_files/corpus.json\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('result_files')\n",
    "except:\n",
    "    print(\"Directory Already Exists\")\n",
    "    \n",
    "with open('result_files/corpus.json', 'w') as corpusfile:  \n",
    "    json.dump(data_text_1, corpusfile)\n",
    "print(\"Corpus File path -> \"+path+'/result_files/corpus.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Fetching relations for all the lemmas in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21949\n",
      "21949\n"
     ]
    }
   ],
   "source": [
    "unique_lemmas = set(corpus_lemma)\n",
    "print(len(unique_lemmas))\n",
    "unique_word_relations = extract_relations(unique_lemmas)\n",
    "print(len(unique_word_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'synset': Synset('headquarters.n.01'), 'relations': {'hypo': ['mukataa'], 'syn': ['headquarters', 'central_office', 'main_office', 'home_office', 'home_base'], 'hyper': ['office', 'business_office'], 'mero': [], 'def': '(usually plural) the office that serves as the administrative center of an enterprise', 'holo': []}}, {'synset': Synset('headquarters.n.02'), 'relations': {'hypo': ['GHQ', 'command_post', 'general_headquarters', 'guardhouse'], 'syn': ['headquarters', 'HQ', 'military_headquarters'], 'hyper': ['military_installation'], 'mero': [], 'def': 'the military installation from which a commander performs the functions of command', 'holo': []}}, {'synset': Synset('headquarters.n.03'), 'relations': {'hypo': ['ACE', 'ACLANT', 'Allied_Command_Atlantic', 'Allied_Command_Europe'], 'syn': ['headquarters'], 'hyper': ['military_unit', 'military_force', 'military_group', 'force'], 'mero': [Synset('headquarters_staff.n.01')], 'def': '(plural) a military unit consisting of a commander and the headquarters staff', 'holo': []}}, {'synset': Synset('headquarter.v.01'), 'relations': {'hypo': [], 'syn': ['headquarter'], 'hyper': ['supply', 'provide', 'render', 'furnish'], 'mero': [], 'def': 'provide with headquarters', 'holo': []}}]\n"
     ]
    }
   ],
   "source": [
    "#TO show to TA\n",
    "print(unique_word_relations['headquarters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'Abraham Thomas Lincoln (February 12, 1809 – April 15, 1865) was an American statesman, politician, and lawyer who served as the 16th president of the United States from 1861 until his assassination in April 1865.', 'ner': ['Abraham Thomas Lincoln_PERSON', 'February 12, 1809_DATE', '– April 15, 1865_DATE', 'American_NORP', '16th_ORDINAL', 'the United States_GPE', '1861_DATE', 'April 1865_DATE'], 'pos': ['Abraham_NNP', 'Thomas_NNP', 'Lincoln_NNP', '(_-LRB-', 'February_NNP', '12_CD', ',_,', '1809_CD', '–_:', 'April_NNP', '15_CD', ',_,', '1865_CD', ')_-RRB-', 'was_VBD', 'an_DT', 'American_JJ', 'statesman_NN', ',_,', 'politician_NN', ',_,', 'and_CC', 'lawyer_NN', 'who_WP', 'served_VBD', 'as_IN', 'the_DT', '16th_JJ', 'president_NN', 'of_IN', 'the_DT', 'United_NNP', 'States_NNP', 'from_IN', '1861_CD', 'until_IN', 'his_PRP$', 'assassination_NN', 'in_IN', 'April_NNP', '1865_CD', '._.'], 'parse_tree': ['Abraham_compound', 'Thomas_compound', 'Lincoln_nsubj', '(_punct', 'February_npadvmod', '12_nummod', ',_punct', '1809_npadvmod', '–_punct', 'April_npadvmod', '15_nummod', ',_punct', '1865_nummod', ')_punct', 'was_ROOT', 'an_det', 'American_amod', 'statesman_attr', ',_punct', 'politician_conj', ',_punct', 'and_cc', 'lawyer_conj', 'who_nsubj', 'served_relcl', 'as_prep', 'the_det', '16th_amod', 'president_pobj', 'of_prep', 'the_det', 'United_compound', 'States_pobj', 'from_prep', '1861_pobj', 'until_prep', 'his_poss', 'assassination_pobj', 'in_prep', 'April_pobj', '1865_nummod', '._punct'], 'lemma': ['Abraham', 'Thomas', 'Lincoln', '(', 'February', '12', ',', '1809', '–', 'April', '15', ',', '1865', ')', 'be', 'an', 'american', 'statesman', ',', 'politician', ',', 'and', 'lawyer', 'who', 'serve', 'as', 'the', '16th', 'president', 'of', 'the', 'United', 'States', 'from', '1861', 'until', '-PRON-', 'assassination', 'in', 'April', '1865', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(data_text_1[0]['sentences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_text_1[0]['sentences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TF- IDF \n",
    "The idea of tf-df was taken from:\n",
    "\n",
    "<b> S. Sareen and S. Sareen, “Process Text using TFIDF in Python,” Towards Data Science, 07-Aug-2018. [Online]. Available: https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8. [Accessed: 08-May-2019]. <b>\n",
    "\n",
    "tf = (frequency of term in the doc/total number of terms in the doc) \n",
    "\n",
    "idf = ln(total number of docs/number of docs with term in it)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    w = extract_words(text)\n",
    "    return (len(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Wikipedia Doc directory\n"
     ]
    }
   ],
   "source": [
    "corpus_words = []\n",
    "path = input(\"Enter Wikipedia Doc directory\")\n",
    "if path == \"\":\n",
    "    path = \"<give some default path name>\"\n",
    "    \n",
    "os.chdir(path)\n",
    "\n",
    "for filename in glob.glob(\"*.txt\"):\n",
    "    if filename != \"MelindaGates.txt\":\n",
    "        f = codecs.open(filename,'r','utf-8-sig')\n",
    "    else:\n",
    "        f = codecs.open(filename,'r','ISO-8859-1')\n",
    "    file = f.read()\n",
    "    len_w = count_words(file)\n",
    "    corpus_words.append({'doc':filename,'count':len_w,'text':file})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def word_frequency(text,doc):\n",
    "    freq_dict={}\n",
    "    words = extract_words(text)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in freq_dict:\n",
    "            freq_dict[word] += 1\n",
    "        else:\n",
    "            freq_dict[word] = 1\n",
    "    temp ={'doc' : doc , 'freq_dict': freq_dict}\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_count_list = []\n",
    "for i in corpus_words:\n",
    "    l = word_frequency(i['text'],i['doc'])\n",
    "    word_count_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculate_TF(corpus_words,word_count_list):\n",
    "    TF_scores =[]\n",
    "    for i in range(0,len(word_count_list)):\n",
    "        id = word_count_list[i]['doc']\n",
    "        for k in word_count_list[i]['freq_dict']:\n",
    "            temp = {'doc': id,\n",
    "                   'TF_score': word_count_list[i]['freq_dict'][k]/corpus_words[i]['count'],\n",
    "                   'key':k}\n",
    "            TF_scores.append(temp)\n",
    "        i += 1\n",
    "    return TF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculate_IDF(corpus_words,word_count_list):\n",
    "    IDF_scores =[]\n",
    "    for dict in word_count_list:\n",
    "        id = dict['doc']\n",
    "        for k in dict['freq_dict'].keys():\n",
    "            count = sum([k in tempDict['freq_dict'] for tempDict in word_count_list])\n",
    "            temp = {'doc': id,'IDF_score': math.log(len(corpus_words)/count),'key' : k}\n",
    "            \n",
    "            IDF_scores.append(temp)\n",
    "    return IDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf = calculate_TF(corpus_words,word_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "idf = calculate_IDF(corpus_words,word_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tf_dataframe = pd.DataFrame(tf)\n",
    "idf_dataframe = pd.DataFrame(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF_score</th>\n",
       "      <th>doc</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001553</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>army</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>unsolicited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000274</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>positions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>oppressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000183</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>prospects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>borne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>resigned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000365</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>physical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TF_score                 doc          key\n",
       "0  0.001553  AbrahamLincoln.txt         army\n",
       "1  0.000091  AbrahamLincoln.txt  unsolicited\n",
       "2  0.000274  AbrahamLincoln.txt    positions\n",
       "3  0.000091  AbrahamLincoln.txt         keep\n",
       "4  0.000091  AbrahamLincoln.txt    oppressed\n",
       "5  0.000183  AbrahamLincoln.txt    prospects\n",
       "6  0.000091  AbrahamLincoln.txt        borne\n",
       "7  0.000091  AbrahamLincoln.txt     resigned\n",
       "8  0.000365  AbrahamLincoln.txt        human\n",
       "9  0.000091  AbrahamLincoln.txt     physical"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF_score</th>\n",
       "      <th>doc</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.098612</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>army</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.401197</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>unsolicited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.455287</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>positions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.836248</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.708050</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>oppressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.708050</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>prospects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.302585</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>borne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.791759</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>resigned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.456758</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.762140</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>physical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDF_score                 doc          key\n",
       "0   1.098612  AbrahamLincoln.txt         army\n",
       "1   3.401197  AbrahamLincoln.txt  unsolicited\n",
       "2   1.455287  AbrahamLincoln.txt    positions\n",
       "3   0.836248  AbrahamLincoln.txt         keep\n",
       "4   2.708050  AbrahamLincoln.txt    oppressed\n",
       "5   2.708050  AbrahamLincoln.txt    prospects\n",
       "6   2.302585  AbrahamLincoln.txt        borne\n",
       "7   1.791759  AbrahamLincoln.txt     resigned\n",
       "8   0.456758  AbrahamLincoln.txt        human\n",
       "9   0.762140  AbrahamLincoln.txt     physical"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tfidf = pd.merge(tf_dataframe, idf_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tfidf['TFIDF_score'] = tfidf['TF_score']*tfidf['IDF_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF_score</th>\n",
       "      <th>doc</th>\n",
       "      <th>key</th>\n",
       "      <th>IDF_score</th>\n",
       "      <th>TFIDF_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001553</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>army</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.001706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>unsolicited</td>\n",
       "      <td>3.401197</td>\n",
       "      <td>0.000311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000274</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>positions</td>\n",
       "      <td>1.455287</td>\n",
       "      <td>0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>keep</td>\n",
       "      <td>0.836248</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>oppressed</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TF_score                 doc          key  IDF_score  TFIDF_score\n",
       "0  0.001553  AbrahamLincoln.txt         army   1.098612     0.001706\n",
       "1  0.000091  AbrahamLincoln.txt  unsolicited   3.401197     0.000311\n",
       "2  0.000274  AbrahamLincoln.txt    positions   1.455287     0.000399\n",
       "3  0.000091  AbrahamLincoln.txt         keep   0.836248     0.000076\n",
       "4  0.000091  AbrahamLincoln.txt    oppressed   2.708050     0.000247"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72981, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tfidf.set_index(\"key\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF_score</th>\n",
       "      <th>doc</th>\n",
       "      <th>IDF_score</th>\n",
       "      <th>TFIDF_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>army</th>\n",
       "      <td>0.001553</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.001706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unsolicited</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>3.401197</td>\n",
       "      <td>0.000311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positions</th>\n",
       "      <td>0.000274</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>1.455287</td>\n",
       "      <td>0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keep</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>0.836248</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oppressed</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>AbrahamLincoln.txt</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TF_score                 doc  IDF_score  TFIDF_score\n",
       "key                                                              \n",
       "army         0.001553  AbrahamLincoln.txt   1.098612     0.001706\n",
       "unsolicited  0.000091  AbrahamLincoln.txt   3.401197     0.000311\n",
       "positions    0.000274  AbrahamLincoln.txt   1.455287     0.000399\n",
       "keep         0.000091  AbrahamLincoln.txt   0.836248     0.000076\n",
       "oppressed    0.000091  AbrahamLincoln.txt   2.708050     0.000247"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TF_score                  doc  IDF_score  TFIDF_score\n",
      "key                                                        \n",
      "at&t  0.000384         AppleInc.txt   1.609438     0.000617\n",
      "at&t  0.046663             AT_T.txt   1.609438     0.075102\n",
      "at&t  0.000688           Dallas.txt   1.609438     0.001107\n",
      "at&t  0.000417              IBM.txt   1.609438     0.000672\n",
      "at&t  0.000261  Rchardson_Texas.txt   1.609438     0.000420\n",
      "at&t  0.000074            Texas.txt   1.609438     0.000118\n"
     ]
    }
   ],
   "source": [
    "#testing for the TF-IDF\n",
    "m = \"at&t\"\n",
    "if m in tfidf.index:\n",
    "    print(tfidf.loc[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corpus_words = {}\n",
    "for filename in glob.glob(\"*.txt\"):\n",
    "    w = []\n",
    "    if filename != \"MelindaGates.txt\":\n",
    "        f = codecs.open(filename,'r','utf-8-sig')\n",
    "    else:\n",
    "        f = codecs.open(filename,'r','ISO-8859-1')\n",
    "    file = f.read()\n",
    "    corpus_words[filename] = file.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def classify_questions(q):\n",
    "    question_type= \"\"\n",
    "    if q != \"\":\n",
    "        if re.search('who',q,re.IGNORECASE):\n",
    "            question_type = 'who'\n",
    "        elif re.search('when',q,re.IGNORECASE):\n",
    "            question_type = 'when'\n",
    "        elif re.search('where',q,re.IGNORECASE):\n",
    "            question_type = 'where'\n",
    "        else :\n",
    "            return(0)\n",
    "    return(question_type)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#implement task 1 on the question\n",
    "def get_nlp_features(q):\n",
    "    words = extract_words(q)\n",
    "    lemmas = extract_lemma(q)\n",
    "    pos = extract_pos(q)\n",
    "    ner = extract_ne(q)\n",
    "    tree = extract_parse_tree(q)\n",
    "    return ({\"sentence\":q,\"lemma\":lemmas,\"pos\":pos,\"ner\": ner,\"parse_tree\":tree,\"words\":words})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf_score(word_list):\n",
    "    ques_list = ['who','when','where']\n",
    "    score_data ={}\n",
    "    for i in word_list:\n",
    "        m = i.lower()\n",
    "        if m in tfidf.index and m not in ques_list:\n",
    "            l = tfidf.loc[m]\n",
    "            try:\n",
    "                if l.dtype == 'object':\n",
    "                    l = pd.DataFrame(tfidf.loc[m]).T\n",
    "            except:\n",
    "                l = l.sort_values(by=['TFIDF_score'],ascending=False)\n",
    "            \n",
    "            files = list(l['doc'].iloc[:3])\n",
    "            scores = list(l['TFIDF_score'].iloc[:3])\n",
    "            score_data[i] =  [files,scores]\n",
    "    return score_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rule1_filter(corpus,lemma_list):\n",
    "    filter1 = []\n",
    "    for i in corpus:\n",
    "        common_terms = list(set(i['lemma']) & set(lemma_list)) \n",
    "        if len(common_terms) > 0:\n",
    "            filter1.append(i)\n",
    "            \n",
    "    print(\"Lines Filtered from rule1_filter------> \",len(filter1))\n",
    "    return filter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rule2_filter(corpus,pos):\n",
    "    filter2 = []\n",
    "    verb = []\n",
    "    for i in pos:\n",
    "        if i.split(\"_\")[1] in ['VB','VBD','VBG']:\n",
    "            verb.append(i.split(\"_\")[0])\n",
    "        if i.split(\"_\")[1] in ['VBP','JJ','NN']:\n",
    "            verb.append(i.split(\"_\")[0])\n",
    "    \n",
    "    if \"purchase\" in verb:\n",
    "        verb.append(\"buy\")\n",
    "    \n",
    "    for i in corpus:  \n",
    "        for p in i['lemma']:\n",
    "            for h in verb:\n",
    "                if p.split(\"_\")[0] == h:\n",
    "                    filter2.append(i)\n",
    "    \n",
    "    print(\"Lines Filtered from rule2_filter------> \",len(filter2))\n",
    "    return filter2    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rule3_filter(corpus,qt):\n",
    "    ans_list = []\n",
    "    sentences = []\n",
    "    if qt == \"who\":\n",
    "        max_score = 0\n",
    "        for i in corpus:\n",
    "            answers= []\n",
    "            for n in i['ner']:\n",
    "                if n.split(\"_\")[1].lower() in ['person','org'] :\n",
    "                    answers.append(n.split(\"_\")[0])\n",
    "            if len(answers) > 0:\n",
    "                obj = {'sentence':i['sentence'],'answer':answers}\n",
    "                ans_list.append(obj)\n",
    "            \n",
    "    if qt == \"when\":\n",
    "        for i in corpus:\n",
    "            answers=[]\n",
    "            for n in i['ner']:\n",
    "                    if n.split(\"_\")[1].lower() == \"date\":\n",
    "                        answers.append(n.split(\"_\")[0])\n",
    "            \n",
    "            if len(answers) > 0:\n",
    "                obj = {'sentence':i['sentence'],'answer':answers}\n",
    "                ans_list.append(obj)\n",
    "                \n",
    "    if qt == \"where\":\n",
    "        for i in corpus:\n",
    "            answers=[]\n",
    "            for n in i['ner']:\n",
    "                    if n.split(\"_\")[1].lower() in ['gpe','loc']:\n",
    "                        answers.append(n.split(\"_\")[0])\n",
    "            \n",
    "            if len(answers) > 0:\n",
    "                obj = {'sentence':i['sentence'],'answer':answers}\n",
    "                ans_list.append(obj)\n",
    "    \n",
    "    print(\"Lines Filtered from rule3_filter------> \",len(ans_list))\n",
    "    return(ans_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rule4_filter(corpus,q_obj):\n",
    "    head_lines =[]\n",
    "    for i in q_obj['parse_tree']:\n",
    "        if i.split(\"_\")[1] == \"ROOT\":\n",
    "            head_of_question = i.split(\"_\")[0]\n",
    "    print(\"Head of Question ------> \"+head_of_question)\n",
    "    \n",
    "    for i in corpus:\n",
    "        for r in i['parse_tree']:\n",
    "            if r.split(\"_\")[1] == \"ROOT\" and r.split(\"_\")[0] == head_of_question:\n",
    "                head_lines.append(i)\n",
    "                \n",
    "    print(\"Lines Filtered from rule4_filter------> \",len(head_lines))\n",
    "    return(head_lines)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rule5_filter(corpus,lemma,pos):\n",
    "    filter4 = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    syn_list = []\n",
    "    verb_root_list = []\n",
    "    \n",
    "    for i in pos:\n",
    "        if i.split(\"_\")[1] in ['VBD','VBP','VBG','VB']:\n",
    "            verb_lemma = lemmatizer.lemmatize(i.split(\"_\")[0],pos=\"v\")\n",
    "            verb_root_list.append(verb_lemma)\n",
    "        if i.split(\"_\")[1] in ['NN']:\n",
    "            verb_root_list.append(i.split(\"_\")[0])\n",
    "            \n",
    "    u_verb_list = list(set(verb_root_list))\n",
    "    \n",
    "    for i in u_verb_list:\n",
    "        if i in unique_word_relations:\n",
    "                m = unique_word_relations[i]\n",
    "                for l in m:\n",
    "                        syn_list.extend(l['relations']['syn'])\n",
    "                        syn_list.extend(l['relations']['hyper'])\n",
    "                        syn_list.extend(l['relations']['hypo'])\n",
    "            \n",
    "    unique_syn_list = list(set(syn_list))\n",
    "    \n",
    "    for i in corpus:\n",
    "        for k in unique_syn_list:\n",
    "            if k in i['lemma']:\n",
    "                filter4.append(i)\n",
    "                \n",
    "    \n",
    "    print(\"Lines Filtered from rule5_filter------> \",len(filter4))\n",
    "    return filter4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rule6_filter(corpus,ques):\n",
    "    filter_list =[]\n",
    "    filter5 = []\n",
    " \n",
    "    for i in corpus:\n",
    "        common_terms = list((set(list(i['lemma'])) & set(ques)))\n",
    "        filter5.append(len(common_terms))\n",
    "\n",
    "    \n",
    "    max_answers =  sorted(range(len(filter5)), key=lambda i: filter5[i], reverse=True)[:2]\n",
    "    \n",
    "    for i in max_answers:\n",
    "        filter_list.append(corpus[i])\n",
    "        \n",
    "    print(\"Lines Filtered from rule6_filter------> \",len(filter_list))\n",
    "    return filter_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def most_probable_file(score,ner):\n",
    "    max_file = []\n",
    "    max_score = 0\n",
    "    agg_score = {}\n",
    "    for i in ner:\n",
    "        j = i.split(\"_\")[0].split(\" \")\n",
    "        for k in j:\n",
    "            if k in score:\n",
    "                for l in range(len(score[k][0])):\n",
    "                    if score[k][0][l] in agg_score:\n",
    "                        agg_score[score[k][0][l]] += score[k][1][l]\n",
    "                    else:\n",
    "                        agg_score[score[k][0][l]] = score[k][1][l]\n",
    "                        \n",
    "    max_file =  nlargest(1, agg_score, key = agg_score.get)\n",
    "    return(max_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_answer(q_obj,files):\n",
    "    corpus = []\n",
    "    answers = []\n",
    "    \n",
    "    print(\"Most Probable Files ---------> \",files)\n",
    "    \n",
    "    for i in files:\n",
    "        for j in data_text_1:\n",
    "            if j['doc'] == i:\n",
    "                corpus = j['sentences']\n",
    "    \n",
    "        filter1 = rule1_filter(corpus,q_obj['obj']['lemma'])\n",
    "        if filter1 != None:\n",
    "            filter4 = rule5_filter(filter1,q_obj['obj']['lemma'],q_obj['obj']['pos'])\n",
    "            filter2 = rule4_filter(filter4,q_obj['obj'])\n",
    "            if len(filter4) == 0:\n",
    "                filter2 = rule4_filter(filter1,q_obj['obj'])\n",
    "            if len(filter2) == 0:\n",
    "                filter2 = rule2_filter(filter4,q_obj['obj']['pos'])  \n",
    "            if filter2 != None:\n",
    "                filter5 = rule6_filter(filter2,q_obj['obj']['lemma'])\n",
    "                filter3 = rule3_filter(filter5,q_obj['qt'])\n",
    "                obj = {'document':i,'info':filter3}\n",
    "                answers.append(obj)\n",
    "    return(answers)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def adding_semantic_rules(obj):\n",
    "    for i in obj['ner']:\n",
    "        if i.split(\"_\")[0] == \"UTD\":\n",
    "            i.split(\"_\")[0].replace(\"UTD\",\"UT Dallas\")\n",
    "            obj['ner'].append(\"UT Dallas_\"+i.split(\"_\")[1])\n",
    "            obj['ner'].append(\"University of Texas at Dallas_\"+i.split(\"_\")[1])\n",
    "        if i.split(\"_\")[0] == \"Abraham Lincoln\":\n",
    "            obj['ner'].append(\"Lincoln_\"+i.split(\"_\")[1])\n",
    "        if i.split(\"_\")[0] == \"ExxonMobile\":\n",
    "            obj['ner'].append(\"ExxonMobil_\"+i.split(\"_\")[1])\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_json_string(ans_dict,q,doc):\n",
    "    sentences = list(ans_dict.keys())\n",
    "    answers = list(ans_dict.values())\n",
    "    sent_dict = {}\n",
    "    ans_dict ={}\n",
    "    doc_dict ={}\n",
    "    for i in range(len(sentences)):\n",
    "        sent_dict[i+1] = sentences[i]\n",
    "        ans_dict[i+1] = str(\" \".join(answers[i]))\n",
    "        doc_dict[i+1] = doc\n",
    "        \n",
    "    obj = json.dumps({\"Question\":q,\"answers\":ans_dict,\"sentences\":sent_dict,\"documents\":doc_dict},sort_keys=False)\n",
    "    return(obj)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the question file path --->C:/Users/taniy/Desktop/Spring19/CS6320-NLP/Project/que1.txt\n"
     ]
    }
   ],
   "source": [
    "input_file = input(\"Enter the question file path --->\")\n",
    "if input_file == \"\":\n",
    "    input_file = \"<give some default path name>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was 16th president of the United States?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['UnitedStates.txt']\n",
      "Lines Filtered from rule1_filter------>  773\n",
      "Lines Filtered from rule5_filter------>  907\n",
      "Head of Question ------> was\n",
      "Lines Filtered from rule4_filter------>  62\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  1\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  Virginia Gazette\n",
      "Sentence :  The first known publication of the phrase \"United States of America\" was in an anonymous essay in The Virginia Gazette newspaper in Williamsburg, Virginia, on April 6, 1776.\n",
      "Document :  UnitedStates.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "When did Amazon surpass Walmart as the most valuable retailer in the United States by market capitalization?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['Walmart.txt']\n",
      "Lines Filtered from rule1_filter------>  479\n",
      "Lines Filtered from rule5_filter------>  289\n",
      "Head of Question ------> surpass\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  87\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  2\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  1988 October 1989\n",
      "Sentence :  By 1988, Walmart was the most profitable retailer in the U.S., and by October 1989, it had become the largest in terms of revenue.\n",
      "Document :  Walmart.txt\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  the late 1980s\n",
      "Sentence :  1990–2005: Retail rise to multinational status\n",
      "\n",
      "While it was the third-largest retailer in the United States, Walmart was more profitable than rivals Kmart and Sears by the late 1980s.\n",
      "Document :  Walmart.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "Where is Apple's satellite campus?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['AppleInc.txt']\n",
      "Lines Filtered from rule1_filter------>  407\n",
      "Lines Filtered from rule5_filter------>  17\n",
      "Head of Question ------> is\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  16\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  2\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  Sunnyvale California\n",
      "Sentence :  This Apple campus has six buildings that total 850,000 square feet (79,000 m2) and was built in 1993 by Sobrato Development Cos.\n",
      "\n",
      "Apple has a satellite campus in neighboring Sunnyvale, California, where it houses a testing and research laboratory.\n",
      "Document :  AppleInc.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "Who founded AT&T?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['AT_T.txt']\n",
      "Lines Filtered from rule1_filter------>  110\n",
      "Lines Filtered from rule5_filter------>  49\n",
      "Head of Question ------> founded\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  0\n",
      "Lines Filtered from rule6_filter------>  0\n",
      "Lines Filtered from rule3_filter------>  0\n",
      "********************************************************************************\n",
      "When did Warren Buffett begin buying stock in Berkshire Hathaway?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['Berkshire_Hathaway.txt']\n",
      "Lines Filtered from rule1_filter------>  199\n",
      "Lines Filtered from rule5_filter------>  361\n",
      "Head of Question ------> begin\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  103\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  2\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  1962\n",
      "Sentence :  In 1962, Warren Buffett began buying stock in Berkshire Hathaway after noticing a pattern in the price direction of its stock whenever the company closed a mill.\n",
      "Document :  Berkshire_Hathaway.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "Where did the Kuomintang-led government retreat?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['China.txt']\n",
      "Lines Filtered from rule1_filter------>  554\n",
      "Lines Filtered from rule5_filter------>  329\n",
      "Head of Question ------> Where\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  65\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  2\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  the People's Republic of China Mainland China Taiwan\n",
      "Sentence :  The Chinese Civil War resulted in a division of territory in 1949, when the Communist Party of China established the People's Republic of China, a unitary one-party sovereign state on Mainland China, while the Kuomintang-led government retreated to the island of Taiwan.\n",
      "Document :  China.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "Who was appointed CEO of Citibank North America by Weill and Reed?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['CitiGroup.txt']\n",
      "Lines Filtered from rule1_filter------>  292\n",
      "Lines Filtered from rule5_filter------>  369\n",
      "Head of Question ------> appointed\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  0\n",
      "Lines Filtered from rule6_filter------>  0\n",
      "Lines Filtered from rule3_filter------>  0\n",
      "********************************************************************************\n",
      "When was Adams-OnÃ­s Treaty between the United States and Spain defined?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['GeorgeWashington.txt']\n",
      "Lines Filtered from rule1_filter------>  595\n",
      "Lines Filtered from rule5_filter------>  686\n",
      "Head of Question ------> defined\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  0\n",
      "Lines Filtered from rule6_filter------>  0\n",
      "Lines Filtered from rule3_filter------>  0\n",
      "********************************************************************************\n",
      "Where was Elon Musk born?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['ElonMusk.txt']\n",
      "Lines Filtered from rule1_filter------>  105\n",
      "Lines Filtered from rule5_filter------>  122\n",
      "Head of Question ------> born\n",
      "Lines Filtered from rule4_filter------>  1\n",
      "Lines Filtered from rule6_filter------>  1\n",
      "Lines Filtered from rule3_filter------>  1\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  Pretoria Transvaal South Africa Regina Saskatchewan Canada\n",
      "Sentence :  Early life\n",
      "Musk was born on June 28, 1971, in Pretoria, Transvaal, South Africa, the son of Maye Musk (née Haldeman), a model and dietitian from Regina, Saskatchewan, Canada, and Errol Musk, a South African electromechanical engineer, pilot, and sailor.\n",
      "Document :  ElonMusk.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "Who founded SpaceX?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['ElonMusk.txt']\n",
      "Lines Filtered from rule1_filter------>  38\n",
      "Lines Filtered from rule5_filter------>  31\n",
      "Head of Question ------> founded\n",
      "Lines Filtered from rule4_filter------>  6\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  2\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  Musk Space Exploration Technologies SpaceX\n",
      "Sentence :  With US$100 million of his early fortune, Musk founded Space Exploration Technologies, or SpaceX, in May 2002.\n",
      "Document :  ElonMusk.txt\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  Musk SpaceX\n",
      "Sentence :  In May 2002, Musk founded SpaceX, an aerospace manufacturer and space transport services company, of which he is CEO and lead designer.\n",
      "Document :  ElonMusk.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "When did a genetic test determin Oprah Winfrey's matrilineal line?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['OprahWinfrey.txt']\n",
      "Lines Filtered from rule1_filter------>  298\n",
      "Lines Filtered from rule5_filter------>  437\n",
      "Head of Question ------> determin\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  29\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  2\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  2006 today\n",
      "Sentence :  A genetic test in 2006 determined that her matrilineal line originated among the Kpelle ethnic group, in the area that today is Liberia.\n",
      "Document :  OprahWinfrey.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "Where did Oprah Winfrey begin attending Lincoln High School?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['OprahWinfrey.txt']\n",
      "Lines Filtered from rule1_filter------>  248\n",
      "Lines Filtered from rule5_filter------>  311\n",
      "Head of Question ------> begin\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  23\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  2\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  Milwaukee\n",
      "Sentence :  She began attending Lincoln High School in Milwaukee, but after early success in the Upward Bound program, was transferred to the affluent suburban Nicolet High School, where she says her poverty was constantly rubbed in her face as she rode the bus to school with fellow African-Americans, some of whom were servants of her classmates' families.\n",
      "Document :  OprahWinfrey.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "Who was both the youngest news anchor and the first black female news anchor at Nashville's WLAC-TV?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['Amazon_com.txt']\n",
      "Lines Filtered from rule1_filter------>  235\n",
      "Lines Filtered from rule5_filter------>  321\n",
      "Head of Question ------> was\n",
      "Lines Filtered from rule4_filter------>  6\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  2\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  Amazon.com the American Legislative Exchange Council ALEC\n",
      "Sentence :  Amazon.com was a corporate member of the American Legislative Exchange Council (ALEC) until it dropped membership following protests at its shareholders' meeting on May 24, 2012.\n",
      "Document :  Amazon_com.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "When was Richardson city originally incorporated?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['Rchardson_Texas.txt']\n",
      "Lines Filtered from rule1_filter------>  167\n",
      "Lines Filtered from rule5_filter------>  236\n",
      "Head of Question ------> incorporated\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  120\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  2\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  May 2018 1925 1956 November 2015\n",
      "Sentence :  Major employers\n",
      "\n",
      "According to the Richardson Economic Development Partnership's listing on Major Employers (last updated May 2018), the top employers in the city are:\n",
      "Government\n",
      "\n",
      "The city is located in North Central Texas and was originally incorporated in 1925, with the first Charter being adopted in 1956 and the latest revision made in November 2015.\n",
      "Document :  Rchardson_Texas.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "Where does Vent-A-Hood have their corporate headquarter?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  []\n",
      "********************************************************************************\n",
      "Who was Steve Jobs' biological father?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['LaurenePowellJobs.txt']\n",
      "Lines Filtered from rule1_filter------>  34\n",
      "Lines Filtered from rule5_filter------>  40\n",
      "Head of Question ------> was\n",
      "Lines Filtered from rule4_filter------>  2\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  2\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  Laurene Powell\n",
      "Sentence :  Laurene Powell was a new MBA student and sneaked to the front of the lecture and started up a conversation with Jobs, who was seated next to her.\n",
      "Document :  LaurenePowellJobs.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "When did Steve Jobs die?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['LaurenePowellJobs.txt']\n",
      "Lines Filtered from rule1_filter------>  22\n",
      "Lines Filtered from rule5_filter------>  7\n",
      "Head of Question ------> die\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  1\n",
      "Lines Filtered from rule6_filter------>  1\n",
      "Lines Filtered from rule3_filter------>  1\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  October 5, 2011 the age of 56\n",
      "Sentence :  Steve Jobs's death\n",
      "\n",
      "On October 5, 2011, at the age of 56, Steve Jobs, the CEO of Apple, died due to complications from a relapse of his previously treated islet-cell neuroendocrine pancreatic cancer.\n",
      "Document :  LaurenePowellJobs.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "Where did Lincoln family move to out of fear of a milk sickness outbreak?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['AbrahamLincoln.txt']\n",
      "Lines Filtered from rule1_filter------>  563\n",
      "Lines Filtered from rule5_filter------>  639\n",
      "Head of Question ------> move\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  116\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  2\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  Illinois Macon County Decatur\n",
      "Sentence :  In early March 1830, partly out of fear of a milk sickness outbreak, several members of the extended Lincoln family moved west to Illinois, a free state, and settled in Macon County, 10 miles (16 km) west of Decatur.\n",
      "Document :  AbrahamLincoln.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "Who was Steve Jobs' adoptive father?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['LaurenePowellJobs.txt']\n",
      "Lines Filtered from rule1_filter------>  34\n",
      "Lines Filtered from rule5_filter------>  40\n",
      "Head of Question ------> was\n",
      "Lines Filtered from rule4_filter------>  2\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  2\n",
      "--------------------------------------------------------------------------------\n",
      "Answer :  Laurene Powell\n",
      "Sentence :  Laurene Powell was a new MBA student and sneaked to the front of the lecture and started up a conversation with Jobs, who was seated next to her.\n",
      "Document :  LaurenePowellJobs.txt\n",
      "--------------------------------------------------------------------------------\n",
      "********************************************************************************\n",
      "When did Washington D.C.'s population had grown 75% from the previous census?\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Details\n",
      "------------------\n",
      "Most Probable Files --------->  ['GeorgeWashington.txt']\n",
      "Lines Filtered from rule1_filter------>  565\n",
      "Lines Filtered from rule5_filter------>  518\n",
      "Head of Question ------> grown\n",
      "Lines Filtered from rule4_filter------>  0\n",
      "Lines Filtered from rule2_filter------>  5\n",
      "Lines Filtered from rule6_filter------>  2\n",
      "Lines Filtered from rule3_filter------>  0\n",
      "********************************************************************************\n",
      "Final Answers fetched --------->  19\n",
      "Directory Already Exists\n",
      "Result file created at Location C:/Users/taniy/Desktop/Spring19/CS6320-NLP/Project/WikipediaArticles/WikipediaArticles/result_files/Max_Entropy_S19_NLP_Project.json\n"
     ]
    }
   ],
   "source": [
    "f= open(input_file,'r')\n",
    "l = f.read().split('\\n')\n",
    "final_answer_list =[]\n",
    "for i in l:\n",
    "    if len(i)>=1:\n",
    "        question = str(i.strip())\n",
    "        print(question)\n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "        print(\"Processing Details\")\n",
    "        print(\"------------------\")\n",
    "        \n",
    "        qt = classify_questions(question)\n",
    "    \n",
    "        obj = get_nlp_features(question)\n",
    "        \n",
    "        score = get_tfidf_score(obj['words'])\n",
    "       \n",
    "        semantic_obj= adding_semantic_rules(obj)\n",
    "     \n",
    "        file = most_probable_file(score,obj['ner'])\n",
    "        \n",
    "        relations = extract_relations(obj['lemma'])\n",
    "        \n",
    "        question_obj = {'q':question,'qt':qt,'obj':semantic_obj,'relation':relations,'score':score}\n",
    "        \n",
    "        answer = get_answer(question_obj,file)\n",
    "        \n",
    "        for i in answer:\n",
    "            unique_question = {}\n",
    "            for s in i['info']:\n",
    "                if s['sentence'] not in unique_question:\n",
    "                    unique_question[s['sentence']] = s['answer']\n",
    "            final_answer_list.append((create_json_string(unique_question,question,i['document'])))\n",
    "            for f in unique_question:\n",
    "                print(\"--------------------------------------------------------------------------------\")\n",
    "                print(\"Answer : \",\" \".join(unique_question[f]))\n",
    "                print(\"Sentence : \",f)\n",
    "                print(\"Document : \",i['document'])\n",
    "                print(\"--------------------------------------------------------------------------------\")\n",
    "        print(\"********************************************************************************\")\n",
    "        \n",
    "print(\"Final Answers fetched ---------> \",len(final_answer_list))\n",
    "try:\n",
    "    os.mkdir('result_files')\n",
    "except:\n",
    "    print(\"Directory Already Exists\")\n",
    "\n",
    "with open('result_files/Max_Entropy_S19_NLP_Project.json', 'w') as outfile:  \n",
    "    json.dump(final_answer_list, outfile)\n",
    "print(\"Result file created at Location \"+path+\"/result_files/Max_Entropy_S19_NLP_Project.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
